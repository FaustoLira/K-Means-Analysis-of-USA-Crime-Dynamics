---
title: "K-Means Analysis of USA Crime Dynamics"
author: "Fausto Lira"
date: "2024-02-13"
output:
  github_document:
    html_preview: false
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Introduction 

The purpose of this study is to apply K-Means clustering to classify regions of the United States based on levels of violent crime, including murder, assault, and rape. By applying this technique to crime data, we aim to identify distinct groups of states with similar crime intensity.

Although each type of crime impacts society differently, for the educational purpose of this project, we will determine the level of violence based on the average across the different types of crime.

The [USArrests](https://www.rdocumentation.org/packages/datasets/versions/3.6.2/topics/USArrests) is a data set included in *datasets* package in R. It provides, for each of the 50 US states, statistics about assault, murder, rape in arrests per 100,000 residents, and the percentage of the population living in urban areas in 1973.

Variables  

`Murder` - Murder arrests (per 100,000)  
`Assault` - Assault arrests (per 100,000)  
`Rape` - Rape arrests (per 100,000)  
`UrbanPop` - Percent urban population

```{r, message = FALSE, warning=FALSE}
## Packages
library(dplyr)
library(ggplot2)
library(bruceR)
library(corrplot)
library(reshape2)
library(ggfortify)  ## autoplot() function
library(factoextra)
library(cluster)
library(cowplot)
```

### Load the data

```{r}
db <- USArrests
```

```{r}
head(db)
```

```{r}
summary(db)
```

```{r}
glimpse(db)
```
## Check Null Values and Duplicate Rows

```{r}
# check null values
colSums(is.na(db))
```

```{r}
# check duplicates rows
sum(duplicated(db))
```

## Univariate Analysis  

```{r, message = FALSE}
# Charts
p1 <- ggplot(db, aes(x = Murder)) +
  geom_histogram()

p2 <- ggplot(db, aes(x = Assault)) +
  geom_histogram()

p3 <- ggplot(db, aes(x = Rape)) +
  geom_histogram()

p4 <- ggplot(db, aes(x = UrbanPop)) +
  geom_histogram()

plot_grid(p1, p2, p3, p4, ncol = 2, nrow = 2)
```
### Basic Statistics

#### Variable: Murder
```{r}
Describe(db$Murder)
```

#### Variable: Assault
```{r}
Describe(db$Assault)
```

#### Variable: Urban Population
```{r}
Describe(db$UrbanPop)
```

#### Variable: Rape
```{r}
Describe(db$Rape)
```
### Bivariate Analysis

Correlation Heat map
```{r}
corr_mat <- round(cor(db, method = "pearson"),2)

# heatmap(corr_mat)

melted_corr_mat <- melt(corr_mat)

ggplot(data=melted_corr_mat, aes(x=Var1, y=Var2, fill=value)) +
  geom_tile() +
  scale_fill_gradientn(colors = c('red', 'white', 'blue'), limits = c(-1, 1)) +
  geom_text(aes(Var2, Var1, label = value), color = "black", size = 4)
```
Let's analyze the correlation between Murder vs Assault, Assault vs Rape, and Murder vs Rape.

```{r, message=FALSE}
p1 <- ggplot(db, aes(x=Murder, y=Assault)) +
  geom_smooth(method = "lm", se = FALSE, color='red') +
  labs(title = "Murder vs Assault") +
  geom_point()

p2<- ggplot(db, aes(x=Assault, y=Rape)) +
  geom_smooth(method = "lm", se = FALSE, color='red') +
  labs(title = "Assault vs Rape") +
  geom_point()

p3 <- ggplot(db, aes(x=Murder, y=Rape)) +
  geom_smooth(method = "lm", se = FALSE, color='red') +
  labs(title = "Murder vs Rape") +
  geom_point()

plot_grid(p1, p2, p3, ncol = 2, nrow = 2)
```
Although there is a significant correlation between murder and assault, we will keep both since the dataset only contains four columns.

## Data Transformation

Scaling replaces all means with 0 and all standard deviations with 1. This is useful for several reasons:

1. K-means is sensible for outliers and scaling is a way to smooth out the problem.  
2. It also helps the compassion between the variables.   
3. It makes easier the interpretation of k-means (details below).   
```{r}
scaled_DB <- data.frame(scale(db))
```
### The convention for k-means used here is as follows

For each variable Murder, Assault, UrbanPop, and Rape let's assume that:

If the value is 0, it indicates that the value is the average.  

If the value belongs to [-1, 0) it will be considered moderately low (**ML**).   
If the value belongs to [-2, 1) it will be considered very low (**VL**).   
If the value is lesser than -2 it will be considered extremely low (**EL**).    

If the value belongs to (0, 1] it will be considered moderately high (**MH**).    
If the value belongs to (1, 2] it will be considered very high (**VH**).     
If the value is greater than 2 it will be considered extremely high (**EH**).    

For example, consider the row 10.
```{r}
scaled_DB[10,]
```
By the above convention, the murder rate is considered extremely high, the assault rate is considered moderately high, the urban population is considered moderately low, and the rape rate is considered moderately high.

### Distributions After Scaling
```{r, message=FALSE}
p1 <- ggplot(scaled_DB, aes(x = Murder)) +
  geom_histogram()

p2 <- ggplot(scaled_DB, aes(x = Assault)) +
  geom_histogram()

p3 <- ggplot(scaled_DB, aes(x = Rape)) +
  geom_histogram()

p4 <- ggplot(scaled_DB, aes(x = UrbanPop)) +
  geom_histogram()

plot_grid(p1, p2, p3, p4, ncol = 2, nrow = 2)
```

## Optimal Number of Clusters from the Elbow and Silhouette Methods

The elbow and silhouette methods can provide a good estimate of the number of clusters, k. However, the final determination of the number of clusters depends on the interpretation of the data and the specific business problem.

Furthermore, visualizing the clusters through PCA helps evaluate the clustering quality. 

```{r}
# Elbow Method
p1 <- fviz_nbclust(scaled_DB, kmeans, method = "wss", k.max = 10) +
  ggtitle("Elbow Method")

# Average Silhouette Width
p2 <- fviz_nbclust(scaled_DB, kmeans, method = "silhouette", k.max = 10) +
  ggtitle("Average Silhouette Width")

# Arrange plots in a grid with increased vertical space
plot_grid(p1, p2, ncol = 1, nrow=2)

```
  
- By the Elbow Method the number of 6 could be the optimized number of clusters.   

- By the Silhouette Method the number of 2 is the optimized number of clusters.

## K-Means with k = 2, 3, 4, 5, 6

Next we apply the k-mean clustering model for k from 2 to 6.   

One can observe that the first two components explain approximately 89% of the entire dataset, as shown in the charts below.

### k=2
```{r}
kmeans2 <- kmeans(scaled_DB, centers = 2, nstart = 10)
kmeans2
```
```{r}
autoplot(kmeans2, scaled_DB, frame = TRUE)
```
#### Interpretation

There are two clusters 1 and 2 which have the centers below.

| Center | Murder | Assault | Rape | Average | UrbanPop |
|--------|--------|---------|------|---------|----------|
|   1    | -0.67  | -0.68   | -0.56|  -0.64  |  -0.13   |
|   2    |  1.00  |  1.01   | 0.85 |   0.95  |   0.20   |


Analyzing the averages one can conclude that:

- The Cluster 1 has a considered moderately low criminality.  
- The Cluster 2 can be considered moderately high criminality.

### K=3
```{r}
kmeans3 <- kmeans(scaled_DB, centers = 3, nstart = 10)
kmeans3
```
```{r}
autoplot(kmeans3, scaled_DB, frame = TRUE)
```
  
#### Interpretation

There are two clusters 1, 2, and 3 which have the centers below. 

| Center | Murder | Assault | Rape | Average | UrbanPop |
|--------|--------|---------|------|---------|----------|
|   1    |  1.00  |   1.01  | 0.85 |   0.96  |   0.20   |
|   2    | -0.96  |  -1.11  |-0.97 |  -1.01  |  -0.93   |
|   3    | -0.45  |  -0.35  |-0.26 |  -0.35  |   0.48   |


Analyzing the averages one can conclude that:

- The Cluster 1 has a considered moderately high criminality.  
- The Cluster 2 can be considered very low criminality.
- The Cluster 3 can be considered moderate low criminality.

### K=4
```{r}
kmeans4 <- kmeans(scaled_DB, centers = 4, nstart = 10)
kmeans4
```
```{r}
autoplot(kmeans4, scaled_DB, frame = TRUE)
```
  
#### Interpretation

There are two clusters 1, 2, 3, and 4 which have the centers below. 

| Center | Murder | Assault | Rape | Average | UrbanPop |
|--------|--------|---------|------|---------|----------|
|   1    |  1.41  |   0.87  | 0.02 |   0.77  |  -0.81   |
|   2    |  0.70  |   1.04  | 1.28 |   1.01  |   0.72   |
|   3    | -0.49  |  -0.38  |-0.26 |  -0.38  |   0.58   |
|   4    | -0.96  |  -1.11  |-0.97 |  -1.01  |  -0.93   |


Analyzing the averages one can conclude that:

- The Cluster 1 has a considered moderately high criminality.  
- The Cluster 2 has a considered very high criminality.  
- The Cluster 3 can be considered moderately low criminality.
- The Cluster 4 can be considered very low criminality.

### K=5
```{r}
kmeans5 <- kmeans(scaled_DB, centers = 5, nstart = 10)
kmeans5
```

```{r}
autoplot(kmeans5, scaled_DB, frame = TRUE)
```
  
The clusters are overlapping.   

### K=6
```{r}
kmeans6 <- kmeans(scaled_DB, centers = 6, nstart = 10)
kmeans6
```
```{r}
autoplot(kmeans6, scaled_DB, frame = TRUE)
```
  
Overlapping again.

K=4 represents a good choice for k-means since it divides the dataset into 4 disjoint subsets. Therefore, applying the k-means algorithm with k=4 provides the optimal clustering of American regions with respect to violent crimes.